# 2-18 텍스트 토큰화

## 문제

문자열을 파싱해서 토큰화하고 싶다.

## 해결

다음과 같은 문자열이 있다.

```
In [140]: text = 'foo = 23 + 42 * 10'
```

문자열을 토큰화하려면 패턴 매칭 이상의 작업이 필요하다. 패턴을 확인할 방법을 가지고 있어야 한다. 예를 들어, 문자열을 다음과 같은 페어 시퀀스로 바꾸고 싶다

```
tokens = [('NAME', 'foo'), ('EQ', '='), ('NUM', '23'), ('PLUS', '+'),
			('NUM', '42'), ('TIMES', '*'), ('NUM', '10')]
```

이런 나누기 작업을 하기 위해서는 공백을 포함해서 가능한 모든 토큰을 정의해야 한다.

```
In [140]: text = 'foo = 23 + 42 * 10'

In [143]: NAME = r'(?P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)'

In [144]: NUM = r'(?P<NUM>\d+)'

In [145]: PLUS = r'(?P<PLUS>\+)'

In [146]: TIMES = r'(?P<TIMES>\*)'

In [147]: EQ = r'(?P<EQ>=)'

In [148]: WS = r'(?P<WS>\s+)'

In [149]: master_pat = re.compile('|'.join([NAME, NUM, PLUS, TIMES, EQ, WS]))
```

re 패턴에서, 패턴에 이름을 붙이기 위해 ?P<이름>을 사용

scanner() 메소드를 사용한다. 이 메소드는 스캐너 객체를 생성하고 전달 받은 텍스트에 match()를 반복적으로 하나씩 호출한다. 스캐너 객체가 동작하는 모습을 다음 예제를 통해 살펴본다.

```
In [184]: scanner = master_pat.scanner('foo = 42')

In [185]: scanner.match()
Out[185]: <_sre.SRE_Match object; span=(0, 3), match='foo'>

In [186]: _.lastgroup, _.group()
Out[186]: ('NAME', 'foo')

In [187]: scanner.match()
Out[187]: <_sre.SRE_Match object; span=(3, 4), match=' '>

In [188]: _.lastgroup, _.group()
Out[188]: ('WS', ' ')

In [189]: scanner.match()
Out[189]: <_sre.SRE_Match object; span=(4, 5), match='='>

In [190]: _.lastgroup, _.group()
Out[190]: ('EQ', '=')

In [191]: scanner.match()
Out[191]: <_sre.SRE_Match object; span=(5, 6), match=' '>

In [192]: _.lastgroup, _.group()
Out[192]: ('WS', ' ')

In [193]: scanner.match()
Out[193]: <_sre.SRE_Match object; span=(6, 8), match='42'>

In [194]: _.lastgroup, _.group()
Out[194]: ('NUM', '42')

In [195]: scanner.match()

In [196]:
```

간결한 생성자 생성

```
In [196]: from collections import namedtuple

In [197]: Token = namedtuple('Token', ['type', 'value'])

In [201]: def generate_tokens(pat, text):
     ...:     scanner = pat.scanner(text)
     ...:     for m in iter(scanner.match, None):
     ...:         yield Token(m.lastgroup, m.group())
     ...:
     ...:

In [202]: for tok in generate_tokens(master_pat, 'foo = 42'):
     ...:     print(tok)
     ...:
Token(type='NAME', value='foo')
Token(type='WS', value=' ')
Token(type='EQ', value='=')
Token(type='WS', value=' ')
Token(type='NUM', value='42')
```

토큰 스트림을 걸러 내고 싶으면 생성자 함수를 더 많이 정의하거나 생성자 표현식을 사용한다. 예를 들어 모든 공백문을 다음과 같이 걸러 낼 수 있다.

```
In [206]: tokens = (tok for tok in generate_tokens(master_pat, 'foo = 42') if tok.type != 'WS')

In [207]: for tok in tokens:
     ...:     print(tok)
     ...:
Token(type='NAME', value='foo')
Token(type='EQ', value='=')
Token(type='NUM', value='42')
```

# 토론

우선 입력부에 나타나는 모든 텍스트 시퀀스를 re 패턴으로 확인해야 한다. 매칭하지 않는 텍스트가 하나라도 있으면 스캐닝이 거기서 멈춘다. (NAME = r'(?P<NAME>[a-zA-Z_0-9]*)' 실행했을 경우 인피니트 상황이라 계속해서 lastgroup 에 NAME 잡히는 현상도 일어난다)

마스터 정규 표현식의 토큰 순서도 중요하다. 매칭할 때 re는 명시한 순서대로 패턴ㄴ을 매칭한다. 따라서 한 패턴이 다른 패턴의 부분이 되는 경우가 있다면 항상 더 긴패턴을 먼저 넣는다

```
In [208]: LT = r'(?P<LT><)'

In [209]: LE = r'(?P<LE><=)'

In [210]: EQ = r'(?P<EQ>=)'

In [211]: master_pat = re.compile('|'.join([LE, LT, EQ])) # 맞음

In [212]: master_pat = re.compile('|'.join([LT, LE, EQ])) # 틀림
```

마지막으로 패턴이 부분 문자열을 형성하는 경우도 조심해야 한다

```
In [213]: PRINT = r'(?P<PRINT>print)'

In [214]: NAME = r'(?P<NAME>[a-zA-Z_][a-zA-Z_0-9]*)'

In [215]: master_pat = re.compile('|'.join([PRINT, NAME]))

In [217]: for tok in generate_tokens(master_pat, 'printer'):
     ...:     print(tok)
     ...:
Token(type='PRINT', value='print')
Token(type='NAME', value='er')
```